# declare entities
a1.sources = s1
a1.channels = memory-channel
a1.sinks = kafka-avro-sink

# configure source
a1.sources.s1.type = spooldir
a1.sources.s1.channels = memory-channel
a1.sources.s1.spoolDir = /home/vagrant/logs
a1.sources.s1.fileHeader = false
a1.sources.s1.deletePolicy = never
a1.sources.s1.deserializer = LINE
a1.sources.s1.deserializer.maxLineLength = 10000

# configure channel
a1.channels.memory-channel.type = memory
a1.channels.memory-channel.capacity = 100
a1.channels.memory-channel.transactionCapacity = 100

# configure sink
a1.sinks.kafka-avro-sink.type = com.scigility.postlogistics.flume.KafkaAvroSink
a1.sinks.kafka-avro-sink.channel = memory-channel
a1.sinks.kafka-avro-sink.avro.schema.file = /home/vagrant/res/Fehlverhalten_HAEBDK_avro.json
a1.sinks.kafka-avro-sink.parser.class = com.scigility.postlogistics.flume.HaebdkParser_NEU
a1.sinks.kafka-avro-sink.topic = log_tet
a1.sinks.kafka-avro-sink.kafka.batch.size = 200
a1.sinks.kafka-avro-sink.kafka.bootstrap.servers = local.cm5.com:9092
a1.sinks.kafka-avro-sink.kafka.key.serializer = io.confluent.kafka.serializers.KafkaAvroSerializer
a1.sinks.kafka-avro-sink.kafka.value.serializer = io.confluent.kafka.serializers.KafkaAvroSerializer
a1.sinks.kafka-avro-sink.kafka.acks=1
a1.sinks.kafka-avro-sink.kafka.compression.type = snappy
a1.sinks.kafka-avro-sink.kafka.schema.registry.url = http://local.cm5.com:8081
